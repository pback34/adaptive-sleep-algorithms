"""
Export module implementation.

This module contains the ExportModule class which provides functionality for exporting
signals and their metadata to various file formats.
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional, Union
from dataclasses import asdict
import pickle
import warnings
import logging # Ensure logging is imported

from ..core import SignalCollection, SignalData, SignalMetadata, CollectionMetadata


class ExportModule:
    """
    Class for exporting signals and metadata to various formats.
    
    Supports exporting to Excel (.xlsx), CSV (.csv), Pickle (.pkl), and HDF5 (.h5).
    """
    
    SUPPORTED_FORMATS = ["excel", "csv", "pickle", "hdf5"]
    
    def __init__(self, collection: SignalCollection):
        """
        Initialize the ExportModule with a SignalCollection.
        
        Args:
            collection: The SignalCollection containing signals to export.
        """
        self.collection = collection

    def export(self, formats: List[str], output_dir: str, include_combined: bool = False, signals: Optional[List[str]] = None) -> None:
        """
        Export signals and metadata to specified formats.

        Args:
            formats: List of formats to export to. Supported: "excel", "csv", "pickle", "hdf5".
            output_dir: Directory where exported files will be saved.
            include_combined: Whether to include the combined dataframe generated by
                              `combine_aligned_signals` or `align_and_combine_signals`.
            signals: Optional list of specific signal keys to export. If provided,
                     only these signals will be exported individually, and
                     `include_combined` will be ignored for this task.

        Raises:
            ValueError: If an unsupported format is specified or if specified signal keys are not found.
            IOError: If there are issues creating output directory or writing files.
        """
        # Logger is available at module level or within specific methods if needed
        logger = logging.getLogger(__name__) # Keep logger init here for use within this method

        # Validate formats
        for fmt in formats:
            if fmt.lower() not in self.SUPPORTED_FORMATS:
                raise ValueError(f"Unsupported format: {fmt}. Supported formats: {self.SUPPORTED_FORMATS}")

        os.makedirs(output_dir, exist_ok=True)

        export_individual = True
        export_combined = False
        signals_to_process = {}
        metadata_keys_to_include = None # For filtering metadata

        if signals:
            logger.info(f"Exporting specific signals: {signals}")
            export_individual = True # Export the specified signals individually
            export_combined = False # Explicit signals override combined export for this task
            if include_combined:
                logger.warning(f"Both 'signals' and 'include_combined=True' specified for export to '{output_dir}'. Exporting specified signals only.")

            # Resolve signal keys
            resolved_keys = []
            missing_keys = []
            for key_spec in signals:
                if key_spec in self.collection.signals:
                    resolved_keys.append(key_spec)
                else:
                    missing_keys.append(key_spec)

            if missing_keys:
                 # Handle error/warning based on strictness? For now, raise error.
                 raise ValueError(f"Specified signal key(s) not found in collection for export: {missing_keys}")
            if not resolved_keys:
                 raise ValueError(f"No valid signals found for export based on specification: {signals}")

            signals_to_process = {key: self.collection.signals[key] for key in resolved_keys}
            metadata_keys_to_include = resolved_keys # Filter metadata

        elif include_combined:
            logger.info("Exporting combined signal only.")
            export_individual = False # Don't export individual signals
            export_combined = True
            # Include metadata for all non-temporary signals for context
            metadata_keys_to_include = [k for k, s in self.collection.signals.items() if not s.metadata.temporary]

        else:
            logger.info("Exporting all individual non-temporary signals.")
            export_individual = True
            export_combined = False # Default is not to export combined unless requested
            # Select all non-temporary signals
            signals_to_process = {k: s for k, s in self.collection.signals.items() if not s.metadata.temporary}
            if not signals_to_process:
                 logger.warning(f"No non-temporary signals found to export individually to '{output_dir}'.")
                 # If no individual signals, check if combined should be exported as fallback? No, stick to logic.
                 export_individual = False # Ensure flag is false if no signals found

            metadata_keys_to_include = list(signals_to_process.keys()) # Include metadata for all exported signals

        # Serialize metadata based on the determined keys
        serialized_metadata = self._serialize_metadata(signal_keys_to_include=metadata_keys_to_include)

        # Export in each requested format
        for fmt in formats:
            fmt = fmt.lower()
            signals_arg = signals_to_process if export_individual else {}
            try:
                if fmt == "excel":
                    self._export_excel(output_dir, signals_arg, export_combined, serialized_metadata)
                elif fmt == "csv":
                    self._export_csv(output_dir, signals_arg, export_combined, serialized_metadata)
                elif fmt == "pickle":
                    self._export_pickle(output_dir, signals_arg, export_combined, serialized_metadata)
                elif fmt == "hdf5":
                    self._export_hdf5(output_dir, signals_arg, export_combined, serialized_metadata)
            except Exception as e:
                 logger.error(f"Failed to export to format '{fmt}' in directory '{output_dir}': {e}", exc_info=True)
                 # Re-raise or handle based on strictness? Re-raise for now.
                 raise IOError(f"Export to format '{fmt}' failed.") from e


    def _format_timestamp(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Convert timestamp index to a column while preserving datetime objects.
        
        Args:
            df: DataFrame with timestamp index.
            
        Returns:
            DataFrame with timestamp as a column (not index).
        """
        import logging
        import pandas as pd
        
        logger = logging.getLogger(__name__)
        
        # Create a copy to avoid modifying the original DataFrame
        result_df = df.copy()
        
        # Handle DatetimeIndex 
        if isinstance(result_df.index, pd.DatetimeIndex):
            # Sort index first to avoid lexsort warning
            if isinstance(result_df.index, pd.MultiIndex):
                result_df = result_df.sort_index()
                
            # Check for duplicates before resetting index
            if result_df.index.duplicated().any():
                dupes = result_df.index.duplicated().sum()
                logger.warning(f"Found {dupes} duplicate timestamps in index. Keeping first occurrence.")
                result_df = result_df.loc[~result_df.index.duplicated(keep='first')]
                
            # Reset index to convert DatetimeIndex to column, preserving datetime type
            # Set the name of the index column to 'timestamp'
            result_df = result_df.reset_index(names='timestamp')
            logger.debug(f"Reset index to column named 'timestamp', dtype: {result_df['timestamp'].dtype}, TZ: {result_df['timestamp'].dt.tz}")

            # Ensure it's still datetime64[ns, TZ] type
            if not pd.api.types.is_datetime64_any_dtype(result_df['timestamp']):
                 logger.warning(f"Column 'timestamp' is not datetime after reset_index: {result_df['timestamp'].dtype}. Attempting conversion.")
                 try:
                     # Preserve original timezone if possible during conversion
                     original_tz = df.index.tz
                     result_df['timestamp'] = pd.to_datetime(result_df['timestamp'])
                     if original_tz and result_df['timestamp'].dt.tz is None:
                          result_df['timestamp'] = result_df['timestamp'].dt.tz_localize(original_tz)
                     elif original_tz and result_df['timestamp'].dt.tz != original_tz:
                          result_df['timestamp'] = result_df['timestamp'].dt.tz_convert(original_tz)
                     logger.debug(f"Successfully converted timestamp column back to datetime. New dtype: {result_df['timestamp'].dtype}, TZ: {result_df['timestamp'].dt.tz}")
                 except Exception as e:
                     logger.error(f"Could not convert timestamp column back to datetime: {e}")
                     # Decide how to handle - raise error or proceed with potentially incorrect type?
                     # raise ValueError(f"Failed to ensure timestamp column is datetime: {e}") from e

        logger.debug(f"Formatted dataframe for export has {len(result_df)} rows")
        return result_df

    def _serialize_metadata(self, signal_keys_to_include: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Serialize collection and relevant signal metadata to a JSON-compatible format.

        Args:
            signal_keys_to_include: Optional list of signal keys. If provided, only metadata
                                    for these signals will be included. If None, metadata for
                                    all signals in the collection is included.

        Returns:
            Dictionary containing serialized metadata.
        """
        logger = logging.getLogger(__name__) # Keep logger init here for use within this method

        # Helper function to recursively convert non-serializable types
        def make_json_serializable(obj):
            if isinstance(obj, dict):
                return {key: make_json_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [make_json_serializable(item) for item in obj]
            elif isinstance(obj, tuple):
                return [make_json_serializable(item) for item in obj]
            elif hasattr(obj, 'isoformat') and callable(obj.isoformat):  # datetime-like objects
                return obj.isoformat()
            elif hasattr(obj, 'name') and hasattr(obj, 'value') and isinstance(obj.value, str):  # Enum objects
                return obj.name
            elif hasattr(obj, '__dict__'):  # Other custom objects
                return make_json_serializable(obj.__dict__)
            else:
                return obj
                
        # Serialize collection metadata
        collection_metadata = make_json_serializable(asdict(self.collection.metadata))

        # Serialize signal metadata, filtering if necessary
        signals_metadata = {}
        keys_to_iterate = signal_keys_to_include if signal_keys_to_include is not None else self.collection.signals.keys()

        for key in keys_to_iterate:
            if key in self.collection.signals:
                signal = self.collection.signals[key]
                signals_metadata[key] = make_json_serializable(asdict(signal.metadata))
            else:
                # This case should ideally not happen if keys are validated before calling
                logger.warning(f"Signal key '{key}' requested for metadata serialization not found in collection.")

        # Prepare combined metadata
        metadata = {
            "collection": collection_metadata,
            "signals": signals_metadata
        }

        return metadata

    def _export_excel(self, output_dir: str, signals_to_export: Dict[str, SignalData], export_combined: bool, serialized_metadata: Dict[str, Any]) -> None:
        """
        Export signals and metadata to Excel format.

        Args:
            output_dir: Directory where Excel file(s) will be saved.
            signals_to_export: Dictionary of individual signals {key: SignalData} to export.
            export_combined: Whether to export the combined dataframe.
            serialized_metadata: Pre-serialized metadata dictionary.
        """
        logger = logging.getLogger(__name__) # Get logger for this method
        signals_file = os.path.join(output_dir, "signals.xlsx")

        # --- Export Individual Signals to Sheets ---
        # This part writes to the 'signals.xlsx' file
        if signals_to_export:
            logger.info(f"Exporting {len(signals_to_export)} individual signals to Excel sheets in '{signals_file}'.")
            with pd.ExcelWriter(signals_file, engine="openpyxl") as writer:
                has_sheets = False
                for key, signal in signals_to_export.items():
                    try:
                        data = signal.get_data()
                        if data is None or data.empty:
                            logger.warning(f"Signal '{key}' has no data, skipping Excel sheet.")
                            continue
                        if isinstance(data, pd.DataFrame):
                            data_formatted = self._format_timestamp(data)
                            # Excel doesn't support timezone-aware datetimes, remove timezone
                            if 'timestamp' in data_formatted.columns and pd.api.types.is_datetime64_any_dtype(data_formatted['timestamp']):
                                if data_formatted['timestamp'].dt.tz is not None:
                                    data_formatted['timestamp'] = data_formatted['timestamp'].dt.tz_localize(None)
                            # Limit sheet name length for Excel compatibility
                            sheet_name = key[:31]
                            data_formatted.to_excel(writer, sheet_name=sheet_name, index=False)
                            has_sheets = True
                        else:
                            warnings.warn(f"Signal {key} does not have DataFrame data, skipping Excel sheet")
                    except Exception as e:
                        logger.error(f"Failed to export signal '{key}' to Excel sheet: {e}", exc_info=True)
                        warnings.warn(f"Failed to export signal '{key}' to Excel sheet.")

                # Add Metadata sheet to the *same* signals.xlsx file
                metadata_rows = []
                for section, section_data in serialized_metadata.items():
                    if section == 'signals':
                        for sig_key, sig_meta_dict in section_data.items():
                            for k, v in sig_meta_dict.items():
                                value = json.dumps(v) if isinstance(v, (dict, list)) else str(v)
                                metadata_rows.append({"key": f"{section}.{sig_key}.{k}", "value": value})
                    elif isinstance(section_data, dict):
                        for k, v in section_data.items():
                            value = json.dumps(v) if isinstance(v, (dict, list)) else str(v)
                            metadata_rows.append({"key": f"{section}.{k}", "value": value})

                metadata_df = pd.DataFrame(metadata_rows)
                metadata_df.to_excel(writer, sheet_name="Metadata", index=False)

                # Add Info sheet if no data sheets were written
                if not has_sheets:
                    empty_df = pd.DataFrame({'Note': ['No individual signals with DataFrame data found or requested']})
                    empty_df.to_excel(writer, sheet_name="Info", index=False)

        # --- Export Combined Dataframe to Separate File ---
        # This part writes to 'combined.xlsx' if requested
        if export_combined:
            logger.info("Exporting combined dataframe to separate Excel file.")
            combined_file = os.path.join(output_dir, "combined.xlsx")
            combined_df = self.collection.get_stored_combined_dataframe()

            if combined_df is not None and not combined_df.empty:
                logger.info(f"Retrieved stored combined dataframe (shape: {combined_df.shape}) for Excel export.")
                # Format timestamp before writing
                combined_df_formatted = self._format_timestamp(combined_df.copy()) # Work on copy

                # Excel doesn't support timezone-aware datetimes, remove timezone
                if 'timestamp' in combined_df_formatted.columns and pd.api.types.is_datetime64_any_dtype(combined_df_formatted['timestamp']):
                    if combined_df_formatted['timestamp'].dt.tz is not None:
                        logger.debug("Removing timezone information for Excel export.")
                        combined_df_formatted['timestamp'] = combined_df_formatted['timestamp'].dt.tz_localize(None)

                # Ensure no NaN rows are included and index is properly handled
                combined_df_formatted.to_excel(combined_file, na_rep='', index=False) # Use index=False as timestamp is a column
                logger.info(f"Successfully exported combined data to {combined_file}")
            elif combined_df is None:
                logger.warning("Stored combined dataframe not found or generation failed. Skipping combined Excel export.")
                warnings.warn("Stored combined dataframe not found or generation failed. Skipping combined Excel export.")
            else: # combined_df is empty
                logger.warning("Stored combined dataframe is empty. Skipping export of combined Excel file.")
                warnings.warn("Stored combined dataframe is empty. Skipping export of combined Excel file.")

    def _export_csv(self, output_dir: str, signals_to_export: Dict[str, SignalData], export_combined: bool, serialized_metadata: Dict[str, Any]) -> None:
        """
        Export signals and metadata to CSV format.

        Args:
            output_dir: Directory where CSV file(s) will be saved.
            signals_to_export: Dictionary of individual signals {key: SignalData} to export.
            export_combined: Whether to export the combined dataframe.
            serialized_metadata: Pre-serialized metadata dictionary.
        """
        import logging
        logger = logging.getLogger(__name__)

        # Get timestamp format from collection metadata, with fallback
        timestamp_format = getattr(self.collection.metadata, 'timestamp_format', '%Y-%m-%d %H:%M:%S.%f')
        logger.debug(f"Using timestamp format: {timestamp_format}")

        # Export each individual signal if requested
        if signals_to_export:
            signals_dir = os.path.join(output_dir, "signals")
            os.makedirs(signals_dir, exist_ok=True)
            logger.info(f"Exporting {len(signals_to_export)} individual signals to CSV files in '{signals_dir}'.")

            for key, signal in signals_to_export.items():
                try:
                    data = signal.get_data()
                    if data is None or data.empty:
                        logger.warning(f"Signal '{key}' has no data, skipping CSV export.")
                        continue
                    if isinstance(data, pd.DataFrame):
                        logger.debug(f"Exporting signal {key} with {len(data)} rows to CSV")

                        # Check for duplicated timestamps
                        if isinstance(data.index, pd.DatetimeIndex) and data.index.duplicated().any():
                            dupes = data.index.duplicated().sum()
                            logger.warning(f"Signal {key} has {dupes} duplicate timestamps. Keeping first occurrences.")
                            data = data.loc[~data.index.duplicated(keep='first')]

                        # Format the timestamp to be a column not an index
                        data_formatted = self._format_timestamp(data)

                        file_path = os.path.join(signals_dir, f"{key}.csv")

                        # Let pandas handle the timestamp formatting natively
                        logger.debug(f"Exporting with timestamp column dtype: {data_formatted['timestamp'].dtype}")
                        sample_time = data_formatted['timestamp'].iloc[0] if len(data_formatted) > 0 else None
                        logger.debug(f"Sample timestamp before export: {sample_time}")

                        # Use pandas native date_format to ensure milliseconds are included
                        data_formatted.to_csv(file_path, date_format='%Y-%m-%d %H:%M:%S.%f', index=False, na_rep='') # Use index=False

                        logger.debug(f"Exported {len(data_formatted)} rows to {file_path}")
                    else:
                        warnings.warn(f"Signal {key} does not have DataFrame data, skipping CSV export")
                except Exception as e:
                    logger.error(f"Failed to export signal '{key}' to CSV: {e}", exc_info=True)
                    warnings.warn(f"Failed to export signal '{key}' to CSV.")

        # Export metadata (always include metadata relevant to the export task)
        metadata_path = os.path.join(output_dir, "metadata.json")
        try:
            with open(metadata_path, 'w') as f:
                # Use default=str for any remaining non-serializable types
                json.dump(serialized_metadata, f, indent=2, default=str)
            logger.info(f"Exported metadata to {metadata_path}")
        except Exception as e:
            logger.error(f"Failed to export metadata to JSON: {e}", exc_info=True)
            warnings.warn("Failed to export metadata.")

        # Export combined dataframe if requested
        if export_combined:
            combined_file = os.path.join(output_dir, "combined.csv")
            logger.info(f"Attempting to export combined dataframe to {combined_file}")

            # Retrieve the stored combined dataframe
            combined_df = self.collection.get_stored_combined_dataframe()

            if combined_df is not None and not combined_df.empty:
                logger.info(f"Retrieved stored combined dataframe with {len(combined_df)} rows and {len(combined_df.columns)} columns for CSV export.")

                # Check if columns are MultiIndex
                if isinstance(combined_df.columns, pd.MultiIndex):
                    logger.debug("Exporting combined CSV with MultiIndex columns and DatetimeIndex.")
                    # Export directly: pandas handles MultiIndex columns and DatetimeIndex correctly
                    combined_df.to_csv(combined_file, date_format='%Y-%m-%d %H:%M:%S.%f', na_rep='')
                    logger.info(f"Exported combined dataframe with MultiIndex columns and {len(combined_df)} rows to {combined_file}")
                else:
                    logger.debug("Exporting combined CSV with standard columns.")
                    # Format the timestamp to be a column for standard export
                    combined_formatted = self._format_timestamp(combined_df.copy()) # Work on copy
                    logger.debug(f"Combined export timestamp column dtype: {combined_formatted['timestamp'].dtype}")
                    sample_time = combined_formatted['timestamp'].iloc[0] if len(combined_formatted) > 0 else None
                    logger.debug(f"Combined sample timestamp before export: {sample_time}")
                    # Use pandas native date_format
                    combined_formatted.to_csv(combined_file, date_format='%Y-%m-%d %H:%M:%S.%f', na_rep='', index=False) # index=False as timestamp is a column
                    logger.info(f"Exported combined dataframe with standard columns and {len(combined_formatted)} rows to {combined_file}")
            elif combined_df is None:
                logger.warning("Stored combined dataframe not found or generation failed. Skipping combined CSV export.")
                warnings.warn("Stored combined dataframe not found or generation failed. Skipping combined CSV export.")
            else: # combined_df is empty
                logger.warning("Stored combined dataframe is empty. Skipping export of combined CSV file.")
                warnings.warn("Stored combined dataframe is empty. Skipping export of combined CSV file.")

    def _export_pickle(self, output_dir: str, signals_to_export: Dict[str, SignalData], export_combined: bool, serialized_metadata: Dict[str, Any]) -> None:
        """
        Export signals and metadata to Pickle format.

        Args:
            output_dir: Directory where Pickle file(s) will be saved.
            signals_to_export: Dictionary of individual signals {key: SignalData} to export.
            export_combined: Whether to export the combined dataframe.
            serialized_metadata: Pre-serialized metadata dictionary (used for consistency, though pickle stores objects).
        """
        # Export selected signals and metadata in a single pickle file
        pickle_file = os.path.join(output_dir, "signals.pkl")
        logger = logging.getLogger(__name__)

        # Prepare data structure
        export_data = {
            "metadata": serialized_metadata, # Store the serialized version for consistency
            "signals": {}
        }

        # Add individual signal data if requested
        if signals_to_export:
            logger.info(f"Adding {len(signals_to_export)} individual signals to Pickle export.")
            for key, signal in signals_to_export.items():
                try:
                    # Store the actual data object
                    export_data["signals"][key] = signal.get_data()
                except Exception as e:
                    logger.error(f"Failed to get data for signal '{key}' for Pickle export: {e}", exc_info=True)
                    warnings.warn(f"Failed to include signal '{key}' in Pickle export.")

        # Add combined dataframe if requested
        if export_combined:
            logger.info("Including combined dataframe in Pickle export.")
            # Retrieve the stored combined dataframe
            combined_df = self.collection.get_stored_combined_dataframe()
            if combined_df is not None: # Include even if empty, as None indicates failure
                logger.info(f"Adding stored combined dataframe (shape: {combined_df.shape}) to Pickle export.")
                export_data["combined"] = combined_df
            else:
                logger.warning("Stored combined dataframe not found or generation failed. Storing None for 'combined' in Pickle.")
                export_data["combined"] = None # Explicitly set to None if failed

        # Save to pickle file
        logger.info(f"Saving export data to Pickle file: {pickle_file}")
        try:
            with open(pickle_file, 'wb') as f:
                pickle.dump(export_data, f)
        except Exception as e:
            logger.error(f"Failed to save data to Pickle file '{pickle_file}': {e}", exc_info=True)
            raise IOError(f"Failed to save Pickle file.") from e

    def _export_hdf5(self, output_dir: str, signals_to_export: Dict[str, SignalData], export_combined: bool, serialized_metadata: Dict[str, Any]) -> None:
        """
        Export signals and metadata to HDF5 format.

        Args:
            output_dir: Directory where HDF5 file(s) will be saved.
            signals_to_export: Dictionary of individual signals {key: SignalData} to export.
            export_combined: Whether to export the combined dataframe.
            serialized_metadata: Pre-serialized metadata dictionary.
        """
        try:
            import h5py
        except ImportError:
            raise ImportError("h5py is required for HDF5 export. Install with: pip install h5py")
        
        # Export to HDF5 file
        h5_file = os.path.join(output_dir, "signals.h5")
        logger = logging.getLogger(__name__)

        # Use pandas HDFStore for DataFrame data
        try:
            with pd.HDFStore(h5_file, mode='w', complevel=9, complib='zlib') as store: # Add compression
                # Store each individual signal if requested
                if signals_to_export:
                    logger.info(f"Storing {len(signals_to_export)} individual signals in HDF5 file.")
                    for key, signal in signals_to_export.items():
                        try:
                            data = signal.get_data()
                            if data is None or data.empty:
                                logger.warning(f"Signal '{key}' has no data, skipping HDF5 storage.")
                                continue
                            if isinstance(data, pd.DataFrame):
                                store.put(f"signals/{key}", data, format='table', data_columns=True) # Use put for more options
                            else:
                                warnings.warn(f"Signal {key} does not have DataFrame data, skipping HDF5 storage")
                        except Exception as e:
                            logger.error(f"Failed to store signal '{key}' in HDF5: {e}", exc_info=True)
                            warnings.warn(f"Failed to store signal '{key}' in HDF5.")

                # Store combined dataframe if requested
                if export_combined:
                    logger.info("Storing combined dataframe in HDF5 export.")
                    # Retrieve the stored combined dataframe
                    combined_df = self.collection.get_stored_combined_dataframe()
                    if combined_df is not None and not combined_df.empty:
                        logger.info(f"Storing combined dataframe (shape: {combined_df.shape}) to HDF5 key 'combined'.")
                        store.put("combined", combined_df, format='table', data_columns=True) # Use put
                    elif combined_df is None:
                        logger.warning("Stored combined dataframe not found or generation failed. Skipping combined HDF5 storage.")
                    else: # combined_df is empty
                        logger.warning("Stored combined dataframe is empty. Skipping storage in HDF5 file.")
                        # Optionally store an empty dataframe? store.put("combined", pd.DataFrame(), format='table')
                logger.info(f"DataFrames stored in HDF5 file: {h5_file}")
        except Exception as e:
            logger.error(f"Failed to write DataFrame data to HDF5 file '{h5_file}': {e}", exc_info=True)
            raise IOError(f"Failed to write HDF5 data.") from e

        # Store metadata separately using h5py (HDFStore is closed now)
        logger.debug(f"Storing metadata in HDF5 file: {h5_file}")
        try:
            with h5py.File(h5_file, 'a') as f:
                # Convert metadata to JSON string and store
                # Use the pre-serialized metadata passed as argument
                # Use dumps with default=str to handle any remaining non-serializable objects
                metadata_json = json.dumps(serialized_metadata, default=str)
                # Delete existing metadata dataset if it exists before creating new one
                if "metadata" in f:
                    del f["metadata"]
                f.create_dataset("metadata", data=metadata_json.encode('utf-8')) # Encode to bytes
            logger.info(f"Metadata stored in HDF5 file: {h5_file}")
        except Exception as e:
            logger.error(f"Failed to write metadata to HDF5 file '{h5_file}': {e}", exc_info=True)
            # Don't necessarily raise IOError here, as data might be saved
            warnings.warn(f"Failed to write metadata to HDF5 file.")
